{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y45yhDufI5Uo"
      },
      "source": [
        "# **RNN**\n",
        "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55i8db8uI5U3"
      },
      "source": [
        "IMDB sentiment classification task\n",
        "\n",
        "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. IMDB provided a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well. Raw text and already processed bag of words formats are provided.\n",
        "\n",
        "You can download the dataset from http://ai.stanford.edu/~amaas/data/sentiment/  or you can directly use \n",
        "\" from keras.datasets import imdb \" to import the dataset.\n",
        "\n",
        "Few points to be noted:\n",
        "Modules like SimpleRNN, LSTM, Activation layers, Dense layers, Dropout can be directly used from keras\n",
        "For preprocessing, you can use required "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SozhvLNkI5U6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e7e115-9471-4803-fbb1-8916e942dfae"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "vocabulary_size = 5000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\n",
        "print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset with 25000 training samples, 25000 test samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrilwfurI5VA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b960be5f-83c9-49a1-f975-b49461415e65"
      },
      "source": [
        "#the review is stored as a sequence of integers. \n",
        "# These are word IDs that have been pre-assigned to individual words, and the label is an integer\n",
        "\n",
        "print('---review---')\n",
        "print(X_train[0])\n",
        "print('---label---')\n",
        "print(y_train[0])\n",
        "\n",
        "# to get the actual review\n",
        "word2id = imdb.get_word_index()\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "print('---review with words---')\n",
        "print([id2word.get(i, ' ') for i in X_train[0]])\n",
        "print('---label---')\n",
        "print(y_train[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---review---\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "---label---\n",
            "1\n",
            "---review with words---\n",
            "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have', 'after', 'out', 'atmosphere', 'never', 'more', 'room', 'and', 'it', 'so', 'heart', 'shows', 'to', 'years', 'of', 'every', 'never', 'going', 'and', 'help', 'moments', 'or', 'of', 'every', 'chest', 'visual', 'movie', 'except', 'her', 'was', 'several', 'of', 'enough', 'more', 'with', 'is', 'now', 'current', 'film', 'as', 'you', 'of', 'mine', 'potentially', 'unfortunately', 'of', 'you', 'than', 'him', 'that', 'with', 'out', 'themselves', 'her', 'get', 'for', 'was', 'camp', 'of', 'you', 'movie', 'sometimes', 'movie', 'that', 'with', 'scary', 'but', 'and', 'to', 'story', 'wonderful', 'that', 'in', 'seeing', 'in', 'character', 'to', 'of', '70s', 'and', 'with', 'heart', 'had', 'shadows', 'they', 'of', 'here', 'that', 'with', 'her', 'serious', 'to', 'have', 'does', 'when', 'from', 'why', 'what', 'have', 'critics', 'they', 'is', 'you', 'that', \"isn't\", 'one', 'will', 'very', 'to', 'as', 'itself', 'with', 'other', 'and', 'in', 'of', 'seen', 'over', 'and', 'for', 'anyone', 'of', 'and', 'br', \"show's\", 'to', 'whether', 'from', 'than', 'out', 'themselves', 'history', 'he', 'name', 'half', 'some', 'br', 'of', 'and', 'odd', 'was', 'two', 'most', 'of', 'mean', 'for', '1', 'any', 'an', 'boat', 'she', 'he', 'should', 'is', 'thought', 'and', 'but', 'of', 'script', 'you', 'not', 'while', 'history', 'he', 'heart', 'to', 'real', 'at', 'and', 'but', 'when', 'from', 'one', 'bit', 'then', 'have', 'two', 'of', 'script', 'their', 'with', 'her', 'nobody', 'most', 'that', 'with', \"wasn't\", 'to', 'with', 'armed', 'acting', 'watch', 'an', 'for', 'with', 'and', 'film', 'want', 'an']\n",
            "---label---\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6upWxEWI5VC"
      },
      "source": [
        "#pad sequences (write your code here)\n",
        "from keras.preprocessing import sequence\n",
        "# we will pad because all the inputs of RNN should be same size, so we will put max wordlimit as 500\n",
        "maxwords=500\n",
        "X_train = sequence.pad_sequences(X_train, maxwords)\n",
        "X_test = sequence.pad_sequences(X_test, maxwords)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RcCOpeNI5VF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4094c341-686d-497b-8e47-a501ee1f2570"
      },
      "source": [
        "#design a RNN model (write your code)\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout,SimpleRNN\n",
        "\n",
        "model=Sequential()\n",
        "model.add(Embedding(vocabulary_size, 32, input_length=maxwords))\n",
        "model.add(SimpleRNN(100,return_sequences=True))\n",
        "model.add(SimpleRNN(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 500, 32)           160000    \n",
            "                                                                 \n",
            " simple_rnn_2 (SimpleRNN)    (None, 500, 100)          13300     \n",
            "                                                                 \n",
            " simple_rnn_3 (SimpleRNN)    (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 193,501\n",
            "Trainable params: 193,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InQ2TED3I5VI"
      },
      "source": [
        "#train and evaluate your model\n",
        "#choose your loss function and optimizer and mention the reason to choose that particular loss function and optimizer\n",
        "# use accuracy as the evaluation metric\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6A9Q0xmI5VJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "136f7835-022c-4d2f-973b-7b1540cc17de"
      },
      "source": [
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
        "model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "390/390 [==============================] - 555s 1s/step - loss: 0.7004 - accuracy: 0.5222 - val_loss: 0.6878 - val_accuracy: 0.6094\n",
            "Epoch 2/3\n",
            "390/390 [==============================] - 561s 1s/step - loss: 0.6794 - accuracy: 0.5553 - val_loss: 0.6512 - val_accuracy: 0.6719\n",
            "Epoch 3/3\n",
            "390/390 [==============================] - 553s 1s/step - loss: 0.5656 - accuracy: 0.6979 - val_loss: 0.8794 - val_accuracy: 0.4219\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feb82363a90>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTXG__EmI5VM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ced288-77e6-4d38-c716-e8db3493570f"
      },
      "source": [
        "#evaluate the model using model.evaluate()\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.5753999948501587\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1uSo8DgI5VN"
      },
      "source": [
        "# **LSTM**\n",
        "\n",
        "Instead of using a RNN, now try using a LSTM model and compare both of them. Which of those performed better and why ?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk4rLYHwI5VP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8263a42-1821-4ec1-8661-7e4e40642fac"
      },
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "modellstm=Sequential()\n",
        "modellstm.add(Embedding(vocabulary_size, 32, input_length=maxwords))\n",
        "modellstm.add(LSTM(100))\n",
        "modellstm.add(Dense(1, activation='sigmoid'))\n",
        "print(modellstm.summary())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 500, 32)           160000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100)               53200     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD2o9qsOvzRT"
      },
      "source": [
        "modellstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckkAr5VsvzIh",
        "outputId": "1715d22c-69a4-4b70-f7a5-c4ece6866ac5"
      },
      "source": [
        "batch_size = 64\n",
        "num_epochs = 3\n",
        "X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\n",
        "X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\n",
        "modellstm.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "390/390 [==============================] - 34s 78ms/step - loss: 0.4720 - accuracy: 0.7634 - val_loss: 0.4309 - val_accuracy: 0.7188\n",
            "Epoch 2/3\n",
            "390/390 [==============================] - 29s 76ms/step - loss: 0.3357 - accuracy: 0.8607 - val_loss: 0.4157 - val_accuracy: 0.8281\n",
            "Epoch 3/3\n",
            "390/390 [==============================] - 29s 75ms/step - loss: 0.2909 - accuracy: 0.8848 - val_loss: 0.2249 - val_accuracy: 0.9375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7feb8aa15210>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqC63dRYYBee",
        "outputId": "36b74987-826b-4112-8ee2-9e58e00f38b2"
      },
      "source": [
        "scores = modellstm.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.8729599714279175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBtRY9jmI5VQ"
      },
      "source": [
        "Perform Error analysis and explain using few examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfJ9C7PChUVm"
      },
      "source": [
        "We can see that LSTM got 87% accuracy and simpleRNN got 57%, we can say LSTM is performing better than simpleRNN. I have written the explaination at the last section\n",
        "\n",
        "# perfoming error analysis on lstm and simpleRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xkED64Zfy7D"
      },
      "source": [
        "yrnn_pred=model.predict(X_test)\n",
        "yrnn_pred[yrnn_pred>=0.5]=1\n",
        "yrnn_pred[yrnn_pred<0.5]=0\n",
        "ylstm_pred=modellstm.predict(X_test)\n",
        "ylstm_pred[ylstm_pred>=0.5]=1\n",
        "ylstm_pred[ylstm_pred<0.5]=0"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrs2ylDaI5VR"
      },
      "source": [
        "def review(val):\n",
        "  st=\"\"\n",
        "  for i in X_test[val]:\n",
        "    x=id2word.get(i)\n",
        "    if (x!=' ' and x):\n",
        "      st+=x\n",
        "      st+=' '\n",
        "  return st"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZpXDPKU0qpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b98ce3e-ece4-4f32-c054-fb77d4fca87b"
      },
      "source": [
        "k=6\n",
        "for i in range(len(X_test)):\n",
        "  if yrnn_pred[i]!=y_test[i] and ylstm_pred[i] == y_test[i] and k>0:\n",
        "    k-=1\n",
        "    print('---review---')\n",
        "    print(review(i))\n",
        "    print('---label---')\n",
        "    print(y_test[i])\n",
        "    print(f\"lstm model got the label as {int(ylstm_pred[i])}\")\n",
        "    print(f\"Simple RNN model got the label as {int(yrnn_pred[i])}\")\n",
        "    print(\"We can see that lstm predicted better than rnn\")\n",
        "    print()\n",
        "    print(\"---------------------------------------------------\")\n",
        "    print()\n",
        "  if k==0:\n",
        "    break"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---review---\n",
            "the wonder own as by is sequence i i and and to of hollywood br of down and getting boring of ever it sadly sadly sadly i i was then does don't close and after one carry as by are be and all family turn in does as three part in another some to be probably with world and her an have and beginning own as is sequence \n",
            "---label---\n",
            "0\n",
            "lstm model got the label as 0\n",
            "Simple RNN model got the label as 1\n",
            "We can see that lstm predicted better than rnn\n",
            "\n",
            "---------------------------------------------------\n",
            "\n",
            "---review---\n",
            "the of and animation and male it and in and explanation and male take no and and and risk this kill in exploitation is vhs fred in of and be male it mentally who and male watch is popular catch know and it and or kill is and and for and male isn't and male her for would well thousands about and heat as it and to of universe form this did her people and to and of hollywood br of you furthermore who film reading to they of here and male lines enemy not like it of help i i of male their it of time buy treatment for it short in classic to pay is their may comedic make is getting using more he either watched yourself g an br really he judge do 7 to commercial annie make out so told rest you and there movies plot jack this having sidekick to childhood any this so family stopped stunning make his makes your not make present in at and to explanation one bit get still been as \n",
            "---label---\n",
            "0\n",
            "lstm model got the label as 0\n",
            "Simple RNN model got the label as 1\n",
            "We can see that lstm predicted better than rnn\n",
            "\n",
            "---------------------------------------------------\n",
            "\n",
            "---review---\n",
            "the no was how least as on this of dull was with her length that used wont this only middle was nothing can that but is grown them subject when of number memorable can watching doesn't an and characters years in can know that cinema enjoy only lead it that with film ' already for with of how in and of script was rather this is interested and was can laughs blood mother and when it likable mother make is interested mother make his is interested mother sexual was can back horrors mother best second and using though woody then which and of setting in at philip it's values in of warm or of how wanted characters for what have least and have he around get and to performances of how either and in as using really way that her hero this of how put director ask director laughs of setting can't philip 1960s from moments with of see by for after calm about too some br that how most br 70's pictures them most would me shocking has of as on they there's manages in values in an for which old that with is do their that with these cool and even watching around woman lot already with of how lot in used to fair training of and original years from he and later to was then which two was into character that mother like rather was offering was away into to away into at due cast \n",
            "---label---\n",
            "0\n",
            "lstm model got the label as 0\n",
            "Simple RNN model got the label as 1\n",
            "We can see that lstm predicted better than rnn\n",
            "\n",
            "---------------------------------------------------\n",
            "\n",
            "---review---\n",
            "the was body doesn't as most was between that with is had and affect movie was general doesn't of see brosnan was can't or as on with beat some when of human br that with her was after few that she to desperate are that was out vision that to one sick once only acting drive for play look way as on i i but of great like saga are as on movie our is very guy to him love only jack br edition but have he for have away one even women most have than this of on have he away 50s have me an for my those br man acting for have gun are minutes as on i i more it even too for as it is whose on more with so work feel for role and to that thing which gorgeous made they an to no seeing fan role and from welles performances i i that with away five for while great age funniest great to and any to other airplane some film so 2001 someone work say for children in maybe of hour first be level i i only starts lot with no of work say although of it's take have stan that most for way of on himself time settings to away moved in failure of dream correct i i when had wont only second for from subject in double this for funniest scenes has of weren't decent part first of bother set but meaning and i i what have people james in can as on have real away gordon she man and to welles man and this that to at animal in wanting this i i as it is isn't can but especially like asking more acting after his also think entire most as into 3 have society more he great some more of accent br as on like he which up surprisingly to he part even know this acting \n",
            "---label---\n",
            "0\n",
            "lstm model got the label as 0\n",
            "Simple RNN model got the label as 1\n",
            "We can see that lstm predicted better than rnn\n",
            "\n",
            "---------------------------------------------------\n",
            "\n",
            "---review---\n",
            "the castle and obscure it is captures like care but be together they very and is playing not crying belongs and bunch go be references and and other art more not wouldn't and four clips and in be together kind more most all brave burns of late br be watchable called and and to there in warner sense in political of and no and and is and future h samurai debut light of hot that different just very and it guess friends castle and is nudity and for total thrillers br actually and in were complex take of and hence br and and consists and it daily in also be previous are of and to maybe very and some br casting board castle to and isn't portrays and and and of medium and future martial and or of macy chasing and to castle 2 out same did film and moon what all after long be second better of and both that's in political clearly in about very and it is must viewer but production highly their love to loved is around and production on \n",
            "---label---\n",
            "0\n",
            "lstm model got the label as 0\n",
            "Simple RNN model got the label as 1\n",
            "We can see that lstm predicted better than rnn\n",
            "\n",
            "---------------------------------------------------\n",
            "\n",
            "---review---\n",
            "the times her have one is and they'd br clever for horror in successfully had had contains to is playing kill for involved in at times and of mountain br hunting double this is and and so events raise and to and and to be works to them in and seems she have release is buy little to do love an this an no have one is few they as you or is angle and come br change was story might told and better what as with is political controversial change you what have pretty is dark them have had kids in character as you \n",
            "---label---\n",
            "0\n",
            "lstm model got the label as 0\n",
            "Simple RNN model got the label as 1\n",
            "We can see that lstm predicted better than rnn\n",
            "\n",
            "---------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUQld1bzni0v"
      },
      "source": [
        "# Explaination of why LSTM is better than Simple RNN  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoojbwUMnnCR"
      },
      "source": [
        "- We know that RNNs have feedback loops in the recurrent layer. This lets them maintain information in 'memory' over time. But, it will be difficult to train simple RNNs to solve problems that require learning long-term temporal dependencies. This is because the gradient of the loss function decays exponentially with time, this called the vanishing gradient problem. \n",
        "\n",
        "- LSTM is a type of RNN that uses special units in addition to standard units. LSTM units include a 'memory cell' that can maintain information in memory for long periods of time. A set of gates is used to control when information enters the memory, when it's output, and when it's forgotten. \n",
        "\n",
        "- By type architecture, LSTMs learns longer-term dependencies and Simple RNN cannot learn longer-term dependencies,\n",
        "\n",
        "- As we can see the example1 I printed, it has negitivity at first but positivity at last, as Simple RNN doesnt learn longer term dependencies it labels it as positive. but LSTM gives correct label as it leanrs longer term dependencies."
      ]
    }
  ]
}